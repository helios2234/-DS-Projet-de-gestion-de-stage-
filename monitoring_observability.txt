# ============================================
# prometheus/prometheus-values.yaml (Helm)
# ============================================
prometheus:
  prometheusSpec:
    retention: 30d
    retentionSize: "50GB"
    
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          storageClassName: gp3
          resources:
            requests:
              storage: 100Gi
    
    resources:
      requests:
        cpu: 500m
        memory: 2Gi
      limits:
        cpu: 2000m
        memory: 4Gi
    
    # Service monitors
    serviceMonitorSelectorNilUsesHelmValues: false
    serviceMonitorSelector: {}
    
    # Additional scrape configs
    additionalScrapeConfigs:
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__

alertmanager:
  alertmanagerSpec:
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          storageClassName: gp3
          resources:
            requests:
              storage: 10Gi
    
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi

# ============================================
# prometheus/alert-rules.yaml
# ============================================
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: internship-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
spec:
  groups:
    - name: internship-system
      interval: 30s
      rules:
        # High CPU Usage
        - alert: HighCPUUsage
          expr: |
            100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High CPU usage detected"
            description: "CPU usage is above 80% for {{ $labels.instance }}"

        # High Memory Usage
        - alert: HighMemoryUsage
          expr: |
            (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High memory usage detected"
            description: "Memory usage is above 85% for {{ $labels.instance }}"

        # Pod Restart
        - alert: PodRestartingTooMuch
          expr: |
            rate(kube_pod_container_status_restarts_total[15m]) > 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Pod is restarting too frequently"
            description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting frequently"

        # API Latency
        - alert: HighAPILatency
          expr: |
            histogram_quantile(0.95, 
              rate(http_request_duration_seconds_bucket[5m])
            ) > 0.5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High API latency detected"
            description: "95th percentile latency is above 500ms for {{ $labels.service }}"

        # Error Rate
        - alert: HighErrorRate
          expr: |
            rate(http_requests_total{status=~"5.."}[5m]) 
            / 
            rate(http_requests_total[5m]) > 0.05
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High error rate detected"
            description: "Error rate is above 5% for {{ $labels.service }}"

        # Database Connection Pool
        - alert: DatabaseConnectionPoolExhausted
          expr: |
            pg_stat_database_numbackends / pg_settings_max_connections > 0.8
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Database connection pool near exhaustion"
            description: "Using more than 80% of available database connections"

        # Disk Space
        - alert: DiskSpaceRunningLow
          expr: |
            (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 15
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Disk space running low"
            description: "Less than 15% disk space available on {{ $labels.instance }}"

        # Service Down
        - alert: ServiceDown
          expr: |
            up{job=~".*-service"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Service is down"
            description: "{{ $labels.job }} has been down for more than 2 minutes"

# ============================================
# alertmanager/alertmanager-config.yaml
# ============================================
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-config
  namespace: monitoring
stringData:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m
      slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'

    route:
      receiver: 'default'
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 12h
      routes:
        - match:
            severity: critical
          receiver: 'critical'
          continue: true
        - match:
            severity: warning
          receiver: 'warning'

    receivers:
      - name: 'default'
        slack_configs:
          - channel: '#alerts'
            title: 'Alert'
            text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

      - name: 'critical'
        slack_configs:
          - channel: '#critical-alerts'
            title: 'ðŸš¨ CRITICAL ALERT'
            text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        pagerduty_configs:
          - service_key: 'YOUR_PAGERDUTY_KEY'

      - name: 'warning'
        slack_configs:
          - channel: '#warnings'
            title: 'âš ï¸ Warning'
            text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

# ============================================
# grafana/grafana-values.yaml (Helm)
# ============================================
grafana:
  adminPassword: 'CHANGE_ME_IN_PRODUCTION'
  
  persistence:
    enabled: true
    storageClassName: gp3
    size: 10Gi
  
  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      cpu: 500m
      memory: 1Gi
  
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://prometheus-server
          access: proxy
          isDefault: true
        - name: Loki
          type: loki
          url: http://loki:3100
          access: proxy

  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default

  dashboards:
    default:
      kubernetes-cluster:
        gnetId: 7249
        revision: 1
        datasource: Prometheus
      node-exporter:
        gnetId: 1860
        revision: 27
        datasource: Prometheus
      postgresql:
        gnetId: 9628
        revision: 7
        datasource: Prometheus

# ============================================
# loki/loki-values.yaml (Helm)
# ============================================
loki:
  auth_enabled: false
  
  persistence:
    enabled: true
    storageClassName: gp3
    size: 50Gi
  
  config:
    schema_config:
      configs:
        - from: 2023-01-01
          store: boltdb-shipper
          object_store: s3
          schema: v11
          index:
            prefix: loki_index_
            period: 24h
    
    storage_config:
      boltdb_shipper:
        active_index_directory: /data/loki/index
        cache_location: /data/loki/cache
        shared_store: s3
      
      aws:
        s3: s3://us-east-1/internship-loki-logs
        s3forcepathstyle: true
    
    limits_config:
      enforce_metric_name: false
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      ingestion_rate_mb: 10
      ingestion_burst_size_mb: 20
  
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 2Gi

# ============================================
# promtail/promtail-values.yaml (Helm)
# ============================================
promtail:
  config:
    lokiAddress: http://loki:3100/loki/api/v1/push
    
    snippets:
      scrapeConfigs: |
        - job_name: kubernetes-pods
          pipeline_stages:
            - cri: {}
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - source_labels:
                - __meta_kubernetes_pod_controller_name
              regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?
              action: replace
              target_label: __tmp_controller_name
            - source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_name
                - __meta_kubernetes_pod_label_app
                - __tmp_controller_name
                - __meta_kubernetes_pod_name
              regex: ^;*([^;]+)(;.*)?$
              action: replace
              target_label: app
            - source_labels:
                - __meta_kubernetes_pod_label_app_kubernetes_io_component
                - __meta_kubernetes_pod_label_component
              regex: ^;*([^;]+)(;.*)?$
              action: replace
              target_label: component
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_node_name
              target_label: node_name
            - action: replace
              source_labels:
              - __meta_kubernetes_namespace
              target_label: namespace
            - action: replace
              replacement: $1
              separator: /
              source_labels:
              - namespace
              - app
              target_label: job
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_name
              target_label: pod
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_container_name
              target_label: container
            - action: replace
              replacement: /var/log/pods/*$1/*.log
              separator: /
              source_labels:
              - __meta_kubernetes_pod_uid
              - __meta_kubernetes_pod_container_name
              target_label: __path__
            - action: replace
              regex: true/(.*)
              replacement: /var/log/pods/*$1/*.log
              separator: /
              source_labels:
              - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash
              - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash
              - __meta_kubernetes_pod_container_name
              target_label: __path__

  resources:
    requests:
      cpu: 200m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 256Mi

# ============================================
# jaeger/jaeger-values.yaml (Helm)
# ============================================
jaeger:
  strategy: production
  
  collector:
    replicaCount: 2
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 1000m
        memory: 2Gi
    
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 5
      targetCPUUtilizationPercentage: 80
  
  query:
    replicaCount: 2
    resources:
      requests:
        cpu: 250m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1Gi
  
  storage:
    type: elasticsearch
    elasticsearch:
      host: elasticsearch-master
      port: 9200
      scheme: https
      user: elastic
      usePassword: true

# ============================================
# Custom Grafana Dashboard (JSON)
# ============================================
{
  "dashboard": {
    "title": "Internship System - API Metrics",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m])) by (service)",
            "legendFormat": "{{ service }}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Response Time (p95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service))",
            "legendFormat": "{{ service }}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (service) / sum(rate(http_requests_total[5m])) by (service)",
            "legendFormat": "{{ service }}"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Active Pods",
        "targets": [
          {
            "expr": "count(kube_pod_status_phase{phase=\"Running\"}) by (namespace)",
            "legendFormat": "{{ namespace }}"
          }
        ],
        "type": "stat"
      }
    ]
  }
}

# ============================================
# application-metrics.py (Example)
# ============================================
from prometheus_client import Counter, Histogram, Gauge
from functools import wraps
import time

# Define metrics
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint']
)

ACTIVE_REQUESTS = Gauge(
    'http_requests_active',
    'Number of active requests',
    ['service']
)

DB_POOL_SIZE = Gauge(
    'db_connection_pool_size',
    'Database connection pool size',
    ['database']
)

def track_metrics(func):
    """Decorator to track endpoint metrics"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        endpoint = func.__name__
        method = "GET"  # Get from request
        
        ACTIVE_REQUESTS.labels(service="auth-service").inc()
        
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            status = 200  # Get from response
            return result
        except Exception as e:
            status = 500
            raise
        finally:
            duration = time.time() - start_time
            REQUEST_DURATION.labels(method=method, endpoint=endpoint).observe(duration)
            REQUEST_COUNT.labels(method=method, endpoint=endpoint, status=status).inc()
            ACTIVE_REQUESTS.labels(service="auth-service").dec()
    
    return wrapper